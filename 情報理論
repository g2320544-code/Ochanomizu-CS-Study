ハフマン符号の構成と平均符号長　　全体なぜなぜそうなるか何に使うか何に使われるから出されているか
ある文字列 (情報) をハフマン符号化する場合、次の1-3を順にすればOK。
各文字(情報)を出現確率（出現頻度）の高い順に並べ、それぞれを独立したグループと考える
各グループの中で、出現確率が低い2つのグループを1つにしていきながら、グループが1つになるまで二分木を作っていく。
作成した二分木から、各文字ごとの符号を求める
各文字ごとに「符号長（符号の文字数） × 出現確率」を求め、それらを足すことで求めることができます
二分木の各節(ノード)の左側を0、右側を1とする
各文字（A～F）ごとに、以下の操作を実施
木構造の一番上の部分から対応するノードまでたどっていく
たどった際に通った0, 1を左から順番に並べたものを符号にする

情報源　エントロピー

エントロピー
ハフマン符号の構成と平均符号長を求める手順を解説します。
💡 ハフマン符号の構成手順
与えられた離散情報源のシンボル確率の集合は P = \{0.30, 0.20, 0.20, 0.10, 0.10, 0.05, 0.05\} です（合計は 1.00）。
ハフマン符号は、確率の小さいシンボルから順に結合していくという貪欲法（Greedy Algorithm）を用いて構成されます。
ステップ 1: 確率を降順に並べる
まず、シンボルとその確率を降順に並べます。
| シンボル | 確率 |
|---|---|
| S_1 | 0.30 |
| S_2 | 0.20 |
| S_3 | 0.20 |
| S_4 | 0.10 |
| S_5 | 0.10 |
| S_6 | 0.05 |
| S_7 | 0.05 |
ステップ 2: 確率の小さいものから結合
確率の最も小さい2つのシンボル（または中間ノード）を選び、それらを結合して新しいノードを作ります。このプロセスを、最終的に確率が 1.0 の根（ルート）ノードに達するまで繰り返します。
| No. | 結合するノード (確率) | 新しいノードの確率 | 結合後の確率集合 |
|---|---|---|---|
| 1 | (0.05, 0.05) (S_6, S_7) | 0.10 | {0.30, 0.20, 0.20, 0.10, 0.10, 0.10} |
| 2 | (0.10, 0.10) (S_4, S_5) | 0.20 | {0.30, 0.20, 0.20, 0.20, 0.10} (ここでは 0.10 は N_1 のこと) |
| 3 | (0.10, 0.20) (N_1, S_4, S_5) | 0.30 | {0.30, 0.20, 0.20, 0.30} |
| 4 | (0.20, 0.20) (S_2, S_3) | 0.40 | {0.30, 0.30, 0.40} |
| 5 | (0.30, 0.30) (S_1, N_3) | 0.60 | {0.40, 0.60} |
| 6 | (0.40, 0.60) (N_4, N_5) | 1.00 | {1.00} |
ステップ 3: 符号を割り当てる
結合の過程で作られた木（ツリー）の根（確率 1.0）から葉（シンボル）に向かって、左側の枝に \mathbf{0}、右側の枝に \mathbf{1} を割り当てます（割り当て方は逆でも可ですが、一貫性が必要です）。
この結果、各シンボルに対応するハフマン符号と符号長 L_i は以下のようになります。
| シンボル | 確率 P_i | 結合ノード | ハフマン符号 | 符号長 L_i |
|---|---|---|---|---|
| S_1 | 0.30 | N_5 の枝 | 10 | 2 |
| S_2 | 0.20 | N_4 の枝 | 00 | 2 |
| S_3 | 0.20 | N_4 の枝 | 01 | 2 |
| S_4 | 0.10 | N_3 の枝 | 110 | 3 |
| S_5 | 0.10 | N_3 の枝 | 1110 | 4 |
| S_6 | 0.05 | N_1 の枝 | 11110 | 5 |
| S_7 | 0.05 | N_1 の枝 | 11111 | 5 |
> ポイント: 確率が大きいシンボルほど符号長が短くなり、確率が小さいシンボルほど符号長が長くなっていることが分かります。
> 
🧮 平均符号長 L の計算
平均符号長 L は、各シンボルの確率 P_i とその符号長 L_i の積の合計として計算されます。

L = \sum_{i} P_i L_i
L = (0.30 \times 2) + (0.20 \times 2) + (0.20 \times 2) + (0.10 \times 3) + (0.10 \times 4) + (0.05 \times 5) + (0.05 \times 5)
L = 0.60 + 0.40 + 0.40 + 0.30 + 0.40 + 0.25 + 0.25
L = 2.60
✅ 答え
 * ハフマン符号: 上記の表を参照。
 * 平均符号長: 2.60 [ビット/シンボル]
📝 補足：情報源のエントロピー
情報源の平均符号長が達成可能な最小値に近いかどうかを確認するために、情報源のエントロピー H を計算できます。ハフマン符号は、エントロピーに非常に近い平均符号長を与えます。
H = -\sum_{i} P_i \log_2 P_i


H \approx 2.58 \text{ [ビット/シンボル]}

平均符号長 L=2.60 は、エントロピー H \approx 2.58 に非常に近いです。


パリティ検査行列
符号語の長さ（n）
符号語を構成する全ビット数（情報ビット + パリティ検査ビット）は、パリティ検査行列 H の列数 n に等しくなります。
パリティ検査ビットの数（m）
 パリティ検査ビットの数 m は、パリティ検査行列 H の行数に等しくなります。
 情報ビットの数（k）
 情報ビットの数 k は、符号語の長さ n から パリティ検査ビットの数 m を引くことで求められます。
    したがって、符号は**(n, k) 符号**となります。

符号が単一誤り訂正可能であるか否かは、パリティ検査行列 H の列に注目することで判定できます。
判定条件
2元線形符号が単一誤り訂正可能であるための必要十分条件は、パリティ検査行列 H の全ての列が以下の2つの条件を満たすことです。
 * 条件 1: 非ゼロであること
   * H のどの列ベクトルも、すべてゼロのベクトル（ゼロベクトル）であってはならない。
     * 理由：列ベクトルがゼロベクトルである場合、その列に対応する符号語のビットに誤りが発生してもシンドローム s = He^T はゼロのままとなり、誤りが検出できません。
 * 条件 2: 互いに線形独立であること (異なること)
   * H の任意の異なる2つの列ベクトルは、互いに異なるベクトルでなければならない。
     * 理由：もし2つの異なる列 j と l が同じベクトルであった場合、ビット j に発生した単一誤りに対するシンドロームと、ビット l に発生した単一誤りに対するシンドロームが区別できなくなり、どのビットが誤っているかを特定できません（誤り位置が一意に定まらない）。
ハミング符号との関係
単一誤り訂正符号の最もよく知られた例はハミング符号です。符号が単一誤り訂正可能であれば、それは最小ハミング距離が d_{\min} \geq 3 であることを意味します。
まとめ
単一誤り訂正可能であるためには：
> パリティ検査行列 H の列が、
>  * すべて非ゼロであり、かつ
>  * すべて互いに異なる
>    必要があります。

与えられたパリティ検査行列 H から、その符号の符号語すべてを書き上げるには、まずその符号の生成行列 G を求めるのが最も一般的な手順です。
🔑 ステップ 1: パリティ検査行列 H から生成行列 G を求める
与えられた線形符号の符号語 \mathbf{c} は、パリティ検査行列 H を用いて次のように定義されます。
\mathbf{c} H^T = \mathbf{0}
符号が組織符号（Systematic Code）として表される場合、パリティ検査行列 H は通常、以下の形に変形できます。
H = [\mathbf{P}^T \mid I_{m}]
ここで、
 * I_m は m \times m の単位行列（m はパリティ検査ビットの数）
 * \mathbf{P}^T は m \times k の行列（k は情報ビットの数）
この形式のとき、符号の生成行列 G は、以下の形になります。
G = [I_{k} \mid \mathbf{P}]
ここで、I_k は k \times k の単位行列です。
具体的な手順
 * 与えられたパリティ検査行列 H に対して、行基本変形（ガウスの消去法）を適用し、H の右側（または左側）に単位行列 I_m が現れるように標準形 [ \mathbf{P}^T \mid I_m ] に変形します。
 * 標準形に変形できたら、その H の左側の行列が \mathbf{P}^T の転置（\mathbf{P}^T）なので、その転置 \mathbf{P} を求めます。
 * G = [I_{k} \mid \mathbf{P}] を構成します。
📝 ステップ 2: すべての符号語を生成する
符号が (n, k) 線形符号である場合、情報ビットの数 k に対して、符号語の総数は 2^k 個になります。
生成行列 G が求められたら、符号語 \mathbf{c} は、すべての可能な情報ベクトル \mathbf{u}（長さ k の 2^k 通りの 0 と 1 の組み合わせ）を用いて計算されます。
\mathbf{c} = \mathbf{u} G
ここで、
 * \mathbf{u} は長さ k の情報ベクトル（例: k=3 なら \mathbf{u} = [000], [001], \dots, [111] の計 2^3=8 通り）
 * G は k \times n の生成行列
 * 計算はすべて \text{GF}(2) 上（つまり、2を法とする加算、排他的論理和 \oplus）で行います。
具体的な手順
 * k を求め、情報ベクトル \mathbf{u} のすべての組み合わせ（2^k 通り）を書き出します。
 * それぞれの \mathbf{u} に対して、\mathbf{c} = \mathbf{u} G の計算を実行します。
 * 計算結果として得られた 2^k 個のベクトル \mathbf{c} が、その符号のすべての符号語です。
💡 例：(5, 3) 符号の場合
情報ビットの数 k=3、符号語の長さ n=5 の符号を考えます。
 * 符号語の数は 2^3 = 8 個。
 * 生成行列 G が例えば次のように求まったとします。
   
   G = \begin{pmatrix} 1 & 0 & 0 & 1 & 1 \\ 0 & 1 & 0 & 1 & 0 \\ 0 & 0 & 1 & 0 & 1 \end{pmatrix}
| 情報ベクトル \mathbf{u} | 計算 \mathbf{c} = \mathbf{u} G | 符号語 \mathbf{c} |
|---|---|---|
| [0 0 0] | [000] G | [0 0 0 0 0] |
| [1 0 0] | [100] G | [1 0 0 1 1] (Gの1行目) |
| [0 1 0] | [010] G | [0 1 0 1 0] (Gの2行目) |
| [0 0 1] | [001] G | [0 0 1 0 1] (Gの3行目) |
| [1 1 0] | [100] G \oplus [010] G | [1 1 0 0 1] ([10011] \oplus [01010]) |
| [1 0 1] | [100] G \oplus [001] G | [1 0 1 1 0] ([10011] \oplus [00101]) |
| [0 1 1] | [010] G \oplus [001] G | [0 1 1 1 1] ([01010] \oplus [00101]) |
| [1 1 1] | [110] G \oplus [001] G | [1 1 1 0 0] ([11001] \oplus [00101]) |
このように、情報ベクトルの全パターンを生成行列 G にかけることで、すべての符号語を書き出すことができます。

🎓 エントロピーの最大値の導出
N 個の要素からなる完全事象系 X のエントロピー H(X) は、各事象の確率 P(x_i) を用いて以下のように定義されます。
H(X) = -\sum_{i=1}^{N} P(x_i) \log P(x_i)
ここで、 \sum_{i=1}^{N} P(x_i) = 1 の制約があります。
このエントロピーが最大値を取るのは、対数関数に関する重要な不等式（またはラグランジュの未定乗数法）を用いることで示せます。ここでは、より直感的な対数関数の性質を利用した証明を行います。
1. 対数関数の不等式による証明
対数関数 \ln(x) は上に凸の関数です。これに関連する重要な不等式として、情報理論の基本的な不等式が知られています。
任意の正の数 a, b に対し、底が e の場合（自然対数 \ln）で考えると、以下の関係が成り立ちます。
\ln a - \ln b \leq \frac{a}{b} - 1

（一般に \ln x \leq x-1 を x=a/b に適用したもの）
この式を底が 2 の対数 \log に拡張し、\log の底を D に設定して一般化し、情報理論で使いやすいように変形します。
H(X) が最大値 H_{\max} を取るときの確率分布を Q=\{Q(x_i)\}、任意の確率分布を P=\{P(x_i)\} とします。
KL情報量（Kullback–Leibler divergence） D(P \| Q) の定義から、常に D(P \| Q) \geq 0 が成り立ちます。
D(P \| Q) = \sum_{i=1}^{N} P(x_i) \log \left(\frac{P(x_i)}{Q(x_i)}\right) \geq 0
この不等式を用いて、最大のエントロピーが一様分布で得られることを示します。
最大エントロピーの候補
エントロピー H(X) の最大値は、確率が一様である場合、すなわち P(x_i) = \frac{1}{N} のときに得られると仮定します。
このときのエントロピーは H_{\max} は：

H_{\max} = -\sum_{i=1}^{N} \frac{1}{N} \log \left(\frac{1}{N}\right) = -N \cdot \frac{1}{N} \cdot (-\log N) = \log N
任意のエントロピーとの比較
任意の確率分布 P=\{P(x_i)\} に対するエントロピー H(P) と H_{\max} の差を考えます。ここで Q(x_i) = \frac{1}{N} を代入します。
 * まず、KL情報量の定義から D(P \| Q) \geq 0 を展開します。
   
   \sum_{i=1}^{N} P(x_i) \log P(x_i) - \sum_{i=1}^{N} P(x_i) \log Q(x_i) \geq 0
 * Q(x_i) = \frac{1}{N} を代入します。
   
   \sum_{i=1}^{N} P(x_i) \log P(x_i) - \sum_{i=1}^{N} P(x_i) \log \left(\frac{1}{N}\right) \geq 0
 * \log \left(\frac{1}{N}\right) = -\log N であるため、
   
   \sum_{i=1}^{N} P(x_i) \log P(x_i) - \left(-\log N\right) \sum_{i=1}^{N} P(x_i) \geq 0
 * \sum_{i=1}^{N} P(x_i) = 1 を代入します。
   
   \sum_{i=1}^{N} P(x_i) \log P(x_i) + \log N \geq 0
 * 両辺を -\mathbf{1} 倍します。
   
   -\sum_{i=1}^{N} P(x_i) \log P(x_i) \leq \log N
 * エントロピーの定義 H(P) = -\sum_{i=1}^{N} P(x_i) \log P(x_i) を代入すると、
   \mathbf{H(X) \leq \log N}
したがって、N個の要素からなる完全事象系 X のエントロピーの最大値は \mathbf{\log N} であることが示されました。
🎯 等号が成立する場合
上記の証明における基本は、KL情報量がゼロ、すなわち D(P \| Q) = 0 のときに等号が成立することです。
D(P \| Q) がゼロになるのは、以下の条件が成り立つときです。
P(x_i) = Q(x_i) \quad \text{for all } i
この証明では Q(x_i) を一様分布 Q(x_i) = \frac{1}{N} と仮定したため、
> 等号が成立する（エントロピーが最大になる）のは、すべての事象 x_i の発生確率が等しい、すなわち \mathbf{P(x_i) = \frac{1}{N}} （一様分布）のときです。
> 
これは、不確実性が最大である状態に対応します。事象の発生について全く予測がつかない（どの事象も同じくらい起こりうる）とき、情報源は最も大きなエントロピー（平均情報量）を持つ、という直感とも一致します。



